# 텍스트를 위한 인공 신경망

## 순차 데이터와 순환 신경망

### 1. 순차 데이터
* 순차 데이터는 텍스트나 시계열 데이터와 같이 순서에 의미가 있는 데이터를 말함.
* 예시로 단어로 이루어진 문장이나 일별 온도를 기록한 데이터와 같이 순서가 데이터에 중요한 요소일 수 있음.

### 2. 순환 신경망
* 기존에 했던 CNN 은 이전에 사용한 샘플의 계산을 수행하고 나면 해당 샘플은 버려지는데 이를 피드포워드 신경망이라 부르며, 데이터의 흐름이 앞으로만 전달됨.
* 순환 신경망에서는 이전에 처리했던 샘플을 다음 샘플을 처리하는데 재사용할 수 있음.
* 순환 신경망은 일반적인 완전 연결 신경망과 거의 비슷, 하지만 여기서 이전 데이터의 처리 흐름을 순환하는 고리 하나를 추가해줘야 함.
> ### 타임스텝
>   * 하나의 뉴런에 A를 입력했을 때 출력은 Oa
>   * 같은 뉴런에 B를 입력할 때 Oa도 함께 입력하여 나온 출력이 Ob
>   * 같은 뉴런에 C를 입력할 때 Ob도 함께 입력하여 나온 출력이 Oc
>   * Oc 출력에는 A, B, C 모든 입력에 대한 정보가 담겨있음.
>   * 타임스텝이 오래될수록 순환되는 정보는 희미해짐.
> ### 셀
>   * 순환 신경망에서는 특별히 층을 셀이라고 부름.
>   * 한 셀에는 여러 개의 뉴런이 있지만 완전 연결 신경망과 달리 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표시.
> ### 은닉 상태
>   * 셀의 출력을 은닉 상태라고 부름.
>   * 은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트 함수(tanh^2)을 주로 사용.
>   * tanh 함수도 S자 모양을 띠기 때문에 종종 시그모이드 함수라고 부르기도 하지만 시그모이드 함수와 달리 -1 ~ 1 사이의 범위를 가짐.

### 3. 셀의 가중치와 입출력
* 순환 신경망에서 각 셀에 입력된 값은 가중치 Wb와 곱해진다고 가정할 때 모든 셀에서 사용되는 가중치는 같음.
* 가장 첫번째 타임스텝에서 사용되는 이전 은닉 상태는 맨 처음 샘플을 입력할 때는 이전 타임스텝이 없으므로 간단히 모두 0으로 초기화함.
> ### 셀의 가중치 크기 계산
>   1. 입력층에 x1 ~ x4 특성 4개가 존재하고, 순환층에는 r1 ~ r3까지 3개의 뉴런이 있다고 가정.
>   2. 가중치 Wx의 크기는 4 x 3 = 12 로 12개.  
> ### 은닉 상태를 위한 가중치 크기 계산
>   1. r1의 은닉 상태가 다음 타임스텝에 재사용될 때 r1과 r2, r3 모두에게 전달.
>   2. r2의 은닉 상태도 마찬가지로 r1, r2, r3 모두에게 전달.
>   3. r3의 은닉 상태도 같음.
>   4. 이 순환층에서 은닉상태를 위한 가중치 Wb는 3 x 3 = 9 로 9개.
> ### 모델 파라미터 수
>   1. 가중치에 절편을 더하기.
>   2. 예시에는 뉴런마다 하나의 절편이 존재.
>   3. 12 + 9 + 3 = 24 로 24개의 모델 파라미터 존재.
> ### 순환층의 입력
>   * 일반적으로 샘플마다 2개의 차원을 가지며, 하나의 샘플을 하나의 시퀀스라고 말함.
>   * 시퀀스 안에는 여러 개의 아이템이 들어 있으며, 여기에서 시퀀스의 길이가 타임스텝 길이가 됨.
>       > ex) I am a boy => 1개의 샘플안에 4개의 단어가 있고 각 단어는 3개로 표현될 수 있음. (1, 4, 3)
>   * 이런 입력이 순환층을 통과하면 두 번째, 세 번째 차원이 사라지고 순환층의 뉴련 개수만큼 출력됨.
> ### 순환층의 출력
>   * 결론적으로 셀이 모든 타임스텝에서 출력을 만들지만 최종 출력은 마자믹 타임스텝의 은닉 상태만 출력으로 내보냄.
>   * 마치 임력된 시퀀스 길이를 모두 읽어서 정보를 마지막 은닉 상태에 압축하여 전달하는 것처럼 볼 수 있음.
>   * 정보를 기억하는 메모리를 가진다고 표현할 수 있음.
>   * 다중 분류일 경우 출력층에 클래스 개수만큼 뉴런을 두고 활성화 함수를 소프트맥스 사용.
>   * 이진 분류일 경우 출력층에 하나의 뉴런을 두고 활성화 함수를 시그모이드 사용.
>   * 마지막 출력 시 셀의 출력이 1차원이므로 Flatten 클래스로 펼쳐줄 필요가 없고 출력 그대로 밀집층에 사용할 수 있음.

## 순환 신경망으로 IMDB 리뷰 분류하기
* 텍스트 데이터의 경우 단어를 숫자로 바꿔줘야 하며 일반적인 방법으로는 데이터에 등장하는 단어마다 고유한 정수를 부여함.
* 위와 같은 방식으로 분리된 단어들을 토큰이라고 부름.
```python
# 데이터 준비
from tensorflow.keras.datasets import imdb
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)
train_input, val_input, train_target, val_target = train_test_split(train_input, train_target,
                                                                    test_size=0.2,
                                                                    random_state=42)

# pad_sequences는 maxlen 보다 긴 시퀀스는 앞에서부터 토큰을 자름 -> 일반적으로 시퀀스의 뒷부분의 정보가 더 유용하리라 기대하기 때문
# 뒤에서부터 자르고 싶다면 truncating='post' 로 변경
# padding='post' 로 변경하면 패딩(0)을 뒤에서 부터 삽입
train_seq = pad_sequences(train_input, maxlen=100, truncating='pre', padding='pre')
val_seq = pad_sequences(val_input, maxlen=100, truncating='pre', padding='pre')
```

### 1. 순환 신경망 만들기
* 

### 2. 순환 신경망 훈련하기

### 3. 단어 임베딩을 사용하기

## LSTM과 GRU 셀

### 1. LSTM 구조

### 2. LSTM 신경망 훈련

### 3. 순환층에 드롭아웃 적용

### 4. 2개의 층 연결

### 5. GRU 구조

### 6. GRU 신경망 훈련
