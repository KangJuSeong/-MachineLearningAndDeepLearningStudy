# 텍스트를 위한 인공 신경망

## 순차 데이터와 순환 신경망

### 1. 순차 데이터
* 순차 데이터는 텍스트나 시계열 데이터와 같이 순서에 의미가 있는 데이터를 말함.
* 예시로 단어로 이루어진 문장이나 일별 온도를 기록한 데이터와 같이 순서가 데이터에 중요한 요소일 수 있음.

### 2. 순환 신경망
* 기존에 했던 CNN 은 이전에 사용한 샘플의 계산을 수행하고 나면 해당 샘플은 버려지는데 이를 피드포워드 신경망이라 부르며, 데이터의 흐름이 앞으로만 전달됨.
* 순환 신경망에서는 이전에 처리했던 샘플을 다음 샘플을 처리하는데 재사용할 수 있음.
* 순환 신경망은 일반적인 완전 연결 신경망과 거의 비슷, 하지만 여기서 이전 데이터의 처리 흐름을 순환하는 고리 하나를 추가해줘야 함.
> ###1. 타임스텝
>   * 하나의 뉴런에 A를 입력했을 때 출력은 Oa
>   * 같은 뉴런에 B를 입력할 때 Oa도 함께 입력하여 나온 출력이 Ob
>   * 같은 뉴런에 C를 입력할 때 Ob도 함께 입력하여 나온 출력이 Oc
>   * Oc 출력에는 A, B, C 모든 입력에 대한 정보가 담겨있음.
>   * 타임스텝이 오래될수록 순환되는 정보는 희미해짐.
> ###2. 셀
>   * 순환 신경망에서는 특별히 층을 셀이라고 부름.
>   * 한 셀에는 여러 개의 뉴런이 있지만 완전 연결 신경망과 달리 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표시.
> ###3. 은닉 상태
>   * 셀의 출력을 은닉 상태라고 부름.
>   * 은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트 함수(tanh^2)을 주로 사용.
>   * tanh 함수도 S자 모양을 띠기 때문에 종종 시그모이드 함수라고 부르기도 하지만 시그모이드 함수와 달리 -1 ~ 1 사이의 범위를 가짐.

### 3. 셀의 가중치와 입출력
* 순환 신경망에서 각 셀에 입력된 값은 가중치 Wb와 곱해진다고 가정할 때 모든 셀에서 사용되는 가중치는 같음.
* 가장 첫번째 타임스텝에서 사용되는 이전 은닉 상태는 맨 처음 샘플을 입력할 때는 이전 타임스텝이 없으므로 간단히 모두 0으로 초기화함.
> ### 셀의 가중치 크기 계산
>   1. 입력층에 x1 ~ x4 특성 4개가 존재하고, 순환층에는 r1 ~ r3까지 3개의 뉴런이 있다고 가정.
>   2. 가중치 Wx의 크기는 4 x 3 = 12 로 12개.  
> ### 은닉 상태를 위한 가중치 크기 계산
>   1. r1의 은닉 상태가 다음 타임스텝에 재사용될 때 r1과 r2, r3 모두에게 전달.
>   2. r2의 은닉 상태도 마찬가지로 r1, r2, r3 모두에게 전달.
>   3. r3의 은닉 상태도 같음.
>   4. 이 순환층에서 은닉상태를 위한 가중치 Wb는 3 x 3 = 9 로 9개.
> ### 모델 파라미터 수
>   1. 가중치에 절편을 더하기.
>   2. 예시에는 뉴런마다 하나의 절편이 존재.
>   3. 12 + 9 + 3 = 24 로 24개의 모델 파라미터 존재.
## 순환 신경망으로 IMDB 리뷰 분류하기

### 1. 순환 신경망 만들기

### 2. 순환 신경망 훈련하기

### 3. 단어 임베딩을 사용하기

## LSTM과 GRU 셀

### 1. LSTM 구조

### 2. LSTM 신경망 훈련

### 3. 순환층에 드롭아웃 적용

### 4. 2개의 층 연결

### 5. GRU 구조

### 6. GRU 신경망 훈련
